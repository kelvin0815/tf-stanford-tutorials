{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"../../../\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Improving the accuracy of our logistic regression on MNIST\n",
    "\n",
    "We got the accuracy of ~90% on our MNIST dataset with our vanilla model, which is\n",
    "unacceptable. \n",
    "\n",
    "The dataset is basically solved and state of the art models <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\">reach accuracies above\n",
    "99%</a> . \n",
    "\n",
    "You can use whatever loss functions, optimizers, even models that you want, as long as\n",
    "your model is built in TensorFlow. \n",
    "\n",
    "You can save your code in the file q2.py. In the comments,\n",
    "explain what you decided to do, instruction on how to run your code, and report your results.\n",
    "\n",
    "Iâ€™m happy if you can reach 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(os.path.join(DATA_DIR, \"MNIST_data/\"), one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Approach: Three-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Params\n",
    "numTrainingSamples = mnist.train.images.shape[0] # 55000\n",
    "numFeatures = mnist.train.images.shape[1] # 784\n",
    "numClasses = mnist.train.labels.shape[1] # 10\n",
    "\n",
    "# Network Params\n",
    "numHiddenUnits1 = 200\n",
    "numHiddenUnits2 = 200\n",
    "numHiddenUnits3 = 200\n",
    "numEpochs = 10000\n",
    "batchSize = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, numFeatures])\n",
    "y = tf.placeholder(tf.float32, shape=[None, numClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_weight(inputDim, outputDim):\n",
    "    return tf.Variable(tf.truncated_normal([inputDim, outputDim], stddev=0.1, seed=1))\n",
    "\n",
    "def layer_bias(dim):\n",
    "    return tf.Variable(tf.constant(0.1), [dim])\n",
    "\n",
    "def network_layer(layerInput, weight, bias):\n",
    "    layerOutput = tf.nn.relu(tf.matmul(layerInput, weight) + bias)\n",
    "    return layerOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'hidden1': layer_weight(numFeatures, numHiddenUnits1),\n",
    "    'hidden2': layer_weight(numHiddenUnits1, numHiddenUnits2),\n",
    "    'hidden3': layer_weight(numHiddenUnits2, numHiddenUnits3),\n",
    "    'output': layer_weight(numHiddenUnits3, numClasses)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden1': layer_bias(numHiddenUnits1),\n",
    "    'hidden2': layer_bias(numHiddenUnits2),\n",
    "    'hidden3': layer_bias(numHiddenUnits3),\n",
    "    'output': layer_bias(numClasses),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiddenLayer1Output = network_layer(X, weights['hidden1'], biases['hidden1'])\n",
    "hiddenLayer2Output = network_layer(hiddenLayer1Output, weights['hidden2'], biases['hidden2'])\n",
    "hiddenLayer3Output = network_layer(hiddenLayer2Output, weights['hidden3'], biases['hidden3'])\n",
    "finalOutput = network_layer(hiddenLayer3Output, weights['output'], biases['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=finalOutput\n",
    "    )\n",
    ")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training accuracy: 0.21\n",
      "Iteration 2, training accuracy: 0.245\n",
      "Iteration 3, training accuracy: 0.385\n",
      "Iteration 4, training accuracy: 0.325\n",
      "Iteration 5, training accuracy: 0.325\n",
      "Iteration 6, training accuracy: 0.415\n",
      "Iteration 7, training accuracy: 0.42\n",
      "Iteration 8, training accuracy: 0.37\n",
      "Iteration 9, training accuracy: 0.455\n",
      "Iteration 1000, training accuracy: 0.965\n",
      "Iteration 2000, training accuracy: 0.995\n",
      "Iteration 3000, training accuracy: 1.0\n",
      "Iteration 4000, training accuracy: 1.0\n",
      "Iteration 5000, training accuracy: 1.0\n",
      "Iteration 6000, training accuracy: 1.0\n",
      "Iteration 7000, training accuracy: 1.0\n",
      "Iteration 8000, training accuracy: 1.0\n",
      "Iteration 9000, training accuracy: 1.0\n",
      "Iteration 10000, training accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(numEpochs):\n",
    "    batch = mnist.train.next_batch(batch_size=batchSize)\n",
    "    sess.run(optimizer, feed_dict={X: batch[0], y: batch[1]})\n",
    "    if ((i+1)<10 or (i+1)%1000 == 0):\n",
    "        batch_accuracy = accuracy.eval(feed_dict={X: batch[0], y: batch[1]})\n",
    "        print(\"Iteration %s, training accuracy: %s\" % ((i+1), batch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "===== Three-layer Neural Network with 200+200+200 HU on MNIST dataset =====\n",
      "===========================================================================\n",
      "Training accuracy:\t 99.985 %\n",
      "Testing accuracy:\t 97.860 %\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      "[[ 965    0    3    0    1    3    4    1    2    1]\n",
      " [   0 1125    1    1    0    2    1    1    3    1]\n",
      " [   5    2 1001    9    4    0    0    4    7    0]\n",
      " [   2    0    4  995    0    2    0    3    4    0]\n",
      " [   1    0    4    1  964    0    3    2    1    6]\n",
      " [   3    0    0   12    2  867    2    1    3    2]\n",
      " [   4    2    0    1    9    3  936    1    2    0]\n",
      " [   1    6    7    4    3    1    0 1002    2    2]\n",
      " [   4    0    2    6    3    2    1    2  950    4]\n",
      " [   1    2    0    5   10    2    1    4    3  981]]\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = accuracy.eval(feed_dict={\n",
    "    X: mnist.train.images, \n",
    "    y: mnist.train.labels\n",
    "})*100\n",
    "testing_accuracy = accuracy.eval(feed_dict={\n",
    "    X: mnist.test.images, \n",
    "    y: mnist.test.labels\n",
    "})*100\n",
    "\n",
    "\n",
    "mnist_predictions = sess.run(tf.argmax(\n",
    "    tf.nn.softmax(finalOutput), 1\n",
    "), feed_dict={X: mnist.test.images})\n",
    "mnist_labels = sess.run(tf.argmax(mnist.test.labels, 1))\n",
    "\n",
    "print(\"===========================================================================\")\n",
    "print(\"===== Three-layer Neural Network with %s+%s+%s HU on MNIST dataset =====\" % \n",
    "      (numHiddenUnits1, numHiddenUnits2, numHiddenUnits3)\n",
    ")\n",
    "print(\"===========================================================================\")\n",
    "print(\"Training accuracy:\\t %.3f %%\" % training_accuracy)\n",
    "print(\"Testing accuracy:\\t %.3f %%\" % testing_accuracy)\n",
    "\n",
    "print(\"\\nConfusion Matrix: \\n\")\n",
    "print(sess.run(tf.confusion_matrix(labels=mnist_labels, predictions=mnist_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Three-layer Neural Net on the notMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../data/notMNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../data/notMNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../data/notMNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../data/notMNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "not_mnist = input_data.read_data_sets(os.path.join(DATA_DIR, \"notMNIST_data/\"), one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(not_mnist.train.images.shape)\n",
    "print(not_mnist.train.labels.shape)\n",
    "print(not_mnist.test.images.shape)\n",
    "print(not_mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexToCharacter = [\n",
    "    'A', 'B', 'C', 'D', 'E',\n",
    "    'F', 'G', 'H', 'I', 'J'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8jU/wHRcRf24wECbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc20d86450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(not_mnist.test.images[-1].reshape(28, 28))\n",
    "plt.title(indexToCharacter[not_mnist.test.labels[-1].argmax()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training accuracy: 0.17\n",
      "Iteration 2, training accuracy: 0.195\n",
      "Iteration 3, training accuracy: 0.215\n",
      "Iteration 4, training accuracy: 0.2\n",
      "Iteration 5, training accuracy: 0.255\n",
      "Iteration 6, training accuracy: 0.285\n",
      "Iteration 7, training accuracy: 0.24\n",
      "Iteration 8, training accuracy: 0.345\n",
      "Iteration 9, training accuracy: 0.28\n",
      "Iteration 1000, training accuracy: 0.875\n",
      "Iteration 2000, training accuracy: 0.915\n",
      "Iteration 3000, training accuracy: 0.92\n",
      "Iteration 4000, training accuracy: 0.945\n",
      "Iteration 5000, training accuracy: 0.925\n",
      "Iteration 6000, training accuracy: 0.96\n",
      "Iteration 7000, training accuracy: 0.95\n",
      "Iteration 8000, training accuracy: 0.94\n",
      "Iteration 9000, training accuracy: 0.975\n",
      "Iteration 10000, training accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(numEpochs):\n",
    "    batch = not_mnist.train.next_batch(batch_size=batchSize)\n",
    "    sess.run(optimizer, feed_dict={X: batch[0], y: batch[1]})\n",
    "    if ((i+1)<10 or (i+1)%1000 == 0):\n",
    "        batch_accuracy = accuracy.eval(feed_dict={X: batch[0], y: batch[1]})\n",
    "        print(\"Iteration %s, training accuracy: %s\" % ((i+1), batch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "===== Three-layer Neural Network with 200+200+200 HU on notMNIST dataset =====\n",
      "==============================================================================\n",
      "Training accuracy:\t 95.202 %\n",
      "Testing accuracy:\t 93.600 %\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      "[[944   9   0   4   2   0  11  18   8   4]\n",
      " [ 11 938   3  16   6   1   7   8   8   2]\n",
      " [  5   2 941   4  18   4  15   3   8   0]\n",
      " [ 10  17   2 944   2   2   4   5   5   9]\n",
      " [ 12  11  14   3 928   7  18   2   3   2]\n",
      " [  7   1   4   5  12 937  10   2  15   7]\n",
      " [  8   8  18   4   9   8 934   4   5   2]\n",
      " [ 21  15   1   3   5   5   5 940   5   0]\n",
      " [ 18   5   1  18   9   6   7   3 912  21]\n",
      " [ 16   0   1   5   3   8   4   5  16 942]]\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = accuracy.eval(feed_dict={\n",
    "    X: not_mnist.train.images, \n",
    "    y: not_mnist.train.labels\n",
    "})*100\n",
    "testing_accuracy = accuracy.eval(feed_dict={\n",
    "    X: not_mnist.test.images, \n",
    "    y: not_mnist.test.labels\n",
    "})*100\n",
    "\n",
    "not_mnist_predictions = sess.run(tf.argmax(\n",
    "    tf.nn.softmax(finalOutput), 1\n",
    "), feed_dict={X: not_mnist.test.images})\n",
    "\n",
    "not_mnist_labels = sess.run(tf.argmax(not_mnist.test.labels, 1))\n",
    "\n",
    "print(\"==============================================================================\")\n",
    "print(\"===== Three-layer Neural Network with %s+%s+%s HU on notMNIST dataset =====\" % \n",
    "      (numHiddenUnits1, numHiddenUnits2, numHiddenUnits3)\n",
    ")\n",
    "print(\"==============================================================================\")\n",
    "print(\"Training accuracy:\\t %.3f %%\" % training_accuracy)\n",
    "print(\"Testing accuracy:\\t %.3f %%\" % testing_accuracy)\n",
    "\n",
    "print(\"\\nConfusion Matrix: \\n\")\n",
    "print(sess.run(tf.confusion_matrix(labels=not_mnist_labels, predictions=not_mnist_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Logistic regression model on coronary heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
       "0  160    12.00  5.73      23.11      1.0     49    25.30    97.20   52    1\n",
       "1  144     0.01  4.41      28.61      0.0     55    28.87     2.06   63    1\n",
       "2  118     0.08  3.48      32.28      1.0     52    29.14     3.81   46    0\n",
       "3  170     7.50  6.41      38.03      1.0     51    31.99    24.26   58    1\n",
       "4  134    13.60  3.50      27.78      1.0     60    25.99    57.34   49    1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease = pd.read_csv(os.path.join(DATA_DIR, \"heart.csv\"))\n",
    "heart_disease['famhist'] = heart_disease['famhist'].apply(\n",
    "    lambda x: 1.0 if x=='Present' else 0.0\n",
    ")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_disease_data = heart_disease.as_matrix(\n",
    "    ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
    ")\n",
    "heart_disease_data = normalize(heart_disease_data)\n",
    "\n",
    "heart_disease_target = np.array(\n",
    "    [[0., 1.] if ele==1 else [1., 0.] for ele in heart_disease['chd']], \n",
    "    dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78536336,  0.05890225,  0.02812583,  0.11343592,  0.00490852,\n",
       "        0.24051753,  0.12418558,  0.47710824,  0.25524309])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((462, 9), (462, 2))\n"
     ]
    }
   ],
   "source": [
    "print(heart_disease_data.shape, heart_disease_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    heart_disease_data, \n",
    "    heart_disease_target, \n",
    "    train_size=0.8, \n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numFeatures = X_train.shape[1]\n",
    "numClasses = y_train.shape[1]\n",
    "\n",
    "numEpochs = 50000\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "y = tf.placeholder(tf.float32, [None, numClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([numFeatures, numClasses], stddev=0.1, seed=1))\n",
    "b = tf.Variable(tf.constant(0.1), [numClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_prediction = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=y_prediction\n",
    "    )\n",
    ")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training accuracy: 0.365854\n",
      "Iteration 2, training accuracy: 0.639566\n",
      "Iteration 3, training accuracy: 0.639566\n",
      "Iteration 4, training accuracy: 0.639566\n",
      "Iteration 5, training accuracy: 0.639566\n",
      "Iteration 6, training accuracy: 0.639566\n",
      "Iteration 7, training accuracy: 0.639566\n",
      "Iteration 8, training accuracy: 0.639566\n",
      "Iteration 9, training accuracy: 0.639566\n",
      "Iteration 5000, training accuracy: 0.680217\n",
      "Iteration 10000, training accuracy: 0.699187\n",
      "Iteration 15000, training accuracy: 0.712737\n",
      "Iteration 20000, training accuracy: 0.715447\n",
      "Iteration 25000, training accuracy: 0.720867\n",
      "Iteration 30000, training accuracy: 0.715447\n",
      "Iteration 35000, training accuracy: 0.718157\n",
      "Iteration 40000, training accuracy: 0.720867\n",
      "Iteration 45000, training accuracy: 0.720867\n",
      "Iteration 50000, training accuracy: 0.718157\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(numEpochs):\n",
    "    sess.run(optimizer, feed_dict={X: X_train, y: y_train})\n",
    "    if ((i+1)<10 or (i+1)%5000==0):\n",
    "        training_accuracy = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "        print(\"Iteration %s, training accuracy: %s\" % ((i+1), training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "===== Logistic Regression on coronary heart disease dataset =====\n",
      "=================================================================\n",
      "Training accuracy:\t 71.816 %\n",
      "Testing accuracy:\t 72.043 %\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      "[[58  8]\n",
      " [18  9]]\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = accuracy.eval(feed_dict={\n",
    "    X: X_train, \n",
    "    y: y_train\n",
    "})*100\n",
    "testing_accuracy = accuracy.eval(feed_dict={\n",
    "    X: X_test, \n",
    "    y: y_test\n",
    "})*100\n",
    "\n",
    "predictions = sess.run(tf.argmax(\n",
    "    tf.nn.softmax(y_prediction), 1\n",
    "), feed_dict={X: X_test})\n",
    "\n",
    "labels = sess.run(tf.argmax(y_test, 1))\n",
    "\n",
    "print(\"=================================================================\")\n",
    "print(\"===== Logistic Regression on coronary heart disease dataset =====\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training accuracy:\\t %.3f %%\" % training_accuracy)\n",
    "print(\"Testing accuracy:\\t %.3f %%\" % testing_accuracy)\n",
    "\n",
    "print(\"\\nConfusion Matrix: \\n\")\n",
    "print(sess.run(tf.confusion_matrix(labels=labels, predictions=predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
